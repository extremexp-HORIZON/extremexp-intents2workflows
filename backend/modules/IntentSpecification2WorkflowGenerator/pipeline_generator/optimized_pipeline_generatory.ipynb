{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Any, List, Dict, Optional, Union, Set, Type\n",
    "import random\n",
    "import math\n",
    "import ast\n",
    "\n",
    "from pyshacl import validate\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(os.path.join('..'))))\n",
    "from common import *\n",
    "from ontology_populator.implementations.core import *\n",
    "from ontology_populator.implementations.knime.knime_implementation import KnimeParameter\n",
    "\n",
    "def get_intent_iri(intent_graph: Graph) -> URIRef:\n",
    "    intent_iri_query = f\"\"\"\n",
    "PREFIX tb: <{tb}>\n",
    "SELECT ?iri\n",
    "WHERE {{\n",
    "    ?iri a tb:Intent .\n",
    "}}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(intent_iri_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['iri']\n",
    "\n",
    "\n",
    "def get_intent_dataset_task(intent_graph: Graph, intent_iri: URIRef) -> Tuple[URIRef, URIRef, URIRef]:\n",
    "    dataset_task_query = f\"\"\"\n",
    "    PREFIX tb: <{tb}>\n",
    "    SELECT ?dataset ?task ?algorithm\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a tb:Intent .\n",
    "        {intent_iri.n3()} tb:overData ?dataset .\n",
    "        ?task tb:tackles {intent_iri.n3()} .\n",
    "        OPTIONAL {{{intent_iri.n3()} tb:specifies ?algorithm}}\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(dataset_task_query).bindings[0]\n",
    "    return result['dataset'], result['task'], result.get('algorithm', None)\n",
    "\n",
    "\n",
    "def get_intent_parameters(intent_graph: Graph):\n",
    "    intent_parameters_query = f\"\"\"\n",
    "    PREFIX tb:<{tb}>\n",
    "    SELECT ?exp_param ?param_val\n",
    "    WHERE{{\n",
    "        ?param_val tb:forParameter ?exp_param .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(intent_parameters_query).bindings\n",
    "    return {param['exp_param']:param['param_val'] for param in result}\n",
    "\n",
    "\n",
    "def get_algorithms_from_task(ontology: Graph, task: URIRef) -> Tuple[URIRef, URIRef]:\n",
    "\n",
    "    algorithm_task_query = f\"\"\"\n",
    "    PREFIX tb: <{tb}>\n",
    "    SELECT ?algorithm\n",
    "    WHERE{{\n",
    "        ?algorithm a tb:Algorithm ;\n",
    "                   tb:solves {task.n3()} .\n",
    "        ?impl tb:implements ?algorithm .\n",
    "        FILTER NOT EXISTS{{\n",
    "            ?algorithm a tb:Algorithm ;\n",
    "                   tb:solves {task.n3()} .\n",
    "            ?impl a tb:ApplierImplementation.\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = ontology.query(algorithm_task_query).bindings\n",
    "    algos = [algo['algorithm'] for algo in result]\n",
    "    return algos\n",
    "\n",
    "\n",
    "def get_exposed_parameters(ontology: Graph, task: URIRef, algorithm: URIRef):\n",
    "\n",
    "    expparams_query = f\"\"\"\n",
    "    PREFIX tb: <{tb}>\n",
    "    SELECT DISTINCT ?exp_param ?label ?value ?condition\n",
    "    WHERE {{\n",
    "        {task.n3()} a tb:Task .\n",
    "        {{\n",
    "            {\"BIND(\" + algorithm.n3() + \" AS ?algorithm) .\" if algorithm else f\"?algorithm tb:solves {task.n3()} .\"}\n",
    "        }}\n",
    "        # {'?algorithm' if algorithm is None else algorithm.n3()} tb:solves {task.n3()} .\n",
    "        ?imp tb:implements ?algorithm .\n",
    "        ?com tb:hasImplementation ?imp ;\n",
    "            tb:exposesParameter ?exp_param .\n",
    "        ?exp_param tb:has_defaultvalue ?value;\n",
    "                tb:has_condition ?condition ;\n",
    "                rdfs:label ?label .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    result = ontology.query(expparams_query).bindings\n",
    "    return result \n",
    "\n",
    "\n",
    "def get_intent_info(intent_graph: Graph, intent_iri: Optional[URIRef] = None) -> \\\n",
    "        Tuple[URIRef, URIRef, List[Dict[str, Any]], URIRef]:\n",
    "    if not intent_iri:\n",
    "        intent_iri = get_intent_iri(intent_graph)\n",
    "\n",
    "    dataset, task, algorithm = get_intent_dataset_task(intent_graph, intent_iri) \n",
    "\n",
    "    return dataset, task, algorithm, intent_iri \n",
    "\n",
    "def get_implementation_input_specs(ontology: Graph, implementation: URIRef) -> List[List[URIRef]]:\n",
    "    input_spec_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} tb:specifiesInput ?spec .\n",
    "            ?spec a tb:DataSpec ;\n",
    "                tb:hasDatatag ?shape ;\n",
    "                tb:has_position ?position .\n",
    "            ?shape a tb:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(input_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def get_implementation_output_specs(ontology: Graph, implementation: URIRef) -> List[List[URIRef]]:\n",
    "    output_spec_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} tb:specifiesOutput ?spec .\n",
    "            ?spec a tb:DataSpec ;\n",
    "                tb:hasDatatag ?shape ;\n",
    "                tb:has_position ?position .\n",
    "            ?shape a tb:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(output_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def flatten_shape(graph: Graph, shape: URIRef) -> List[URIRef]:\n",
    "    if (shape, SH['and'], None) in graph:\n",
    "        subshapes_query = f\"\"\"\n",
    "            PREFIX sh: <{SH}>\n",
    "            PREFIX rdf: <{RDF}>\n",
    "\n",
    "            SELECT ?subshape\n",
    "            WHERE {{\n",
    "                {shape.n3()} sh:and ?andNode .\n",
    "                ?andNode rdf:rest*/rdf:first ?subshape .\n",
    "            }}\n",
    "        \"\"\"\n",
    "        subshapes = graph.query(subshapes_query).bindings\n",
    "\n",
    "        return [x for subshape in subshapes for x in flatten_shape(graph, subshape['subshape'])]\n",
    "    else:\n",
    "        return [shape]\n",
    "\n",
    "\n",
    "def get_all_implementations(ontology: Graph, task_iri: URIRef = None, algorithm: URIRef = None) -> \\\n",
    "        List[Tuple[URIRef, List[URIRef]]]:\n",
    "    main_implementation_query = f\"\"\"\n",
    "    PREFIX tb: <{tb}>\n",
    "    SELECT DISTINCT ?implementation\n",
    "    WHERE {{\n",
    "        ?implementation a tb:Implementation ;\n",
    "            tb:implements {algorithm.n3() if algorithm is not None else '?algorithm'} .\n",
    "        ?algorithm a tb:Algorithm ;\n",
    "            tb:solves {task_iri.n3() if task_iri is not None else '?task'} .\n",
    "        ?subtask tb:subtaskOf* {task_iri.n3() if task_iri is not None else '?task'} .\n",
    "    }}\n",
    "\"\"\"\n",
    "    results = ontology.query(main_implementation_query).bindings\n",
    "    implementations = [result['implementation'] for result in results]\n",
    "\n",
    "    implementations_with_shapes = [\n",
    "        (implementation, get_implementation_input_specs(ontology, implementation))\n",
    "        for implementation in implementations]\n",
    "\n",
    "    return implementations_with_shapes\n",
    "\n",
    "\n",
    "def get_potential_implementations(ontology: Graph, task_iri: URIRef, algorithm: URIRef = None) -> \\\n",
    "        List[Tuple[URIRef, List[URIRef]]]:\n",
    "    main_implementation_query = f\"\"\"\n",
    "    PREFIX tb: <{tb}>\n",
    "    SELECT DISTINCT ?implementation\n",
    "    WHERE {{\n",
    "        ?implementation a tb:Implementation ;\n",
    "            tb:implements {algorithm.n3() if algorithm is not None else '?algorithm'} .\n",
    "        ?algorithm a tb:Algorithm ;\n",
    "            tb:solves {task_iri.n3() if task_iri is not None else '?task'} .\n",
    "        ?subtask tb:subtaskOf* {task_iri.n3() if task_iri is not None else '?task'} .\n",
    "        FILTER NOT EXISTS{{\n",
    "            ?implementation a tb:ApplierImplementation.\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n",
    "    results = ontology.query(main_implementation_query).bindings\n",
    "    implementations = [result['implementation'] for result in results]\n",
    "    # print(f\"FOCUSED IMPL: {implementations}\")\n",
    "\n",
    "    implementations_with_shapes = [\n",
    "        (implementation, get_implementation_input_specs(ontology, implementation))\n",
    "        for implementation in implementations]\n",
    "\n",
    "    return implementations_with_shapes\n",
    "\n",
    "\n",
    "def get_component_implementation(ontology: Graph, component: URIRef) -> URIRef:\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        PREFIX cb: <{cb}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            {component.n3()} tb:hasImplementation ?implementation .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['implementation']\n",
    "\n",
    "def get_implementation_components(ontology: Graph, implementation: URIRef) -> List[URIRef]:\n",
    "    components_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?component\n",
    "        WHERE {{\n",
    "            ?component tb:hasImplementation {implementation.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(components_query).bindings\n",
    "    return [result['component'] for result in results]\n",
    "\n",
    "\n",
    "def find_components_to_satisfy_shape(ontology: Graph, shape: URIRef, exclude_appliers: bool = False) -> List[URIRef]:\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            {{\n",
    "                ?implementation a tb:Implementation;\n",
    "                                tb:specifiesOutput ?spec .\n",
    "            }}\n",
    "            FILTER NOT EXISTS {{\n",
    "                ?implementation a tb:{'Applier' if exclude_appliers else ''}Implementation .\n",
    "                                # tb:specifiesOutput ?spec .\n",
    "            }}\n",
    "            ?spec tb:hasDatatag {shape.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    implementations = [x['implementation'] for x in result]\n",
    "    components = [c\n",
    "                  for implementation in implementations\n",
    "                  for c in get_implementation_components(ontology, implementation)]\n",
    "    return components\n",
    "\n",
    "def identify_data_io(ontology: Graph, ios: List[List[URIRef]], train: bool = False, test: bool = False, return_index: bool = False) -> Union[int, List[URIRef]]:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            if (io_shape, SH.targetClass, dmop.TabularDataset) in ontology or (io_shape, SH.targetClass, cb.TabularDatasetShape):\n",
    "                if test:\n",
    "                    test_query = f'''\n",
    "                    PREFIX sh: <{SH}>\n",
    "                    PREFIX rdfs: <{RDFS}>\n",
    "                    PREFIX cb: <{cb}>\n",
    "                    PREFIX dmop: <{dmop}>\n",
    "\n",
    "                    ASK {{\n",
    "                        {{\n",
    "                        {io_shape.n3()} a sh:NodeShape ;\n",
    "                                        sh:targetClass dmop:TabularDataset ;\n",
    "                                        sh:property [\n",
    "                                            sh:path dmop:isTestDataset ;\n",
    "                                            sh:hasValue true\n",
    "                                        ] .\n",
    "                        }}\n",
    "                    }}\n",
    "                    '''\n",
    "                    result = ontology.query(test_query).askAnswer\n",
    "                    if result:\n",
    "                        return i if return_index else io_shapes\n",
    "                    \n",
    "                if train:\n",
    "                    train_query = f'''\n",
    "                    PREFIX sh: <{SH}>\n",
    "                    PREFIX rdfs: <{RDFS}>\n",
    "                    PREFIX cb: <{cb}>\n",
    "                    PREFIX dmop: <{dmop}>\n",
    "\n",
    "                    ASK {{\n",
    "                        {{\n",
    "                        {io_shape.n3()} a sh:NodeShape ;\n",
    "                                        sh:targetClass dmop:TabularDataset ;\n",
    "                                        sh:property [\n",
    "                                            sh:path dmop:isTrainDataset ;\n",
    "                                            sh:hasValue true\n",
    "                                        ] .\n",
    "                        }}\n",
    "                    }}\n",
    "                    '''\n",
    "                    result = ontology.query(train_query).askAnswer\n",
    "                    if result:\n",
    "                        return i if return_index else io_shapes\n",
    "                \n",
    "                if not train and not test:\n",
    "                    return i if return_index else io_shapes\n",
    "            \n",
    "def identify_model_io(ontology: Graph, ios: List[List[URIRef]], return_index: bool = False) -> Union[int, List[URIRef]]:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            query = f'''\n",
    "    PREFIX sh: <{SH}>\n",
    "    PREFIX rdfs: <{RDFS}>\n",
    "    PREFIX cb: <{cb}>\n",
    "\n",
    "    ASK {{\n",
    "      {{\n",
    "        {io_shape.n3()} sh:targetClass ?targetClass .\n",
    "        ?targetClass rdfs:subClassOf* cb:Model .\n",
    "      }}\n",
    "      UNION\n",
    "      {{\n",
    "        {io_shape.n3()} rdfs:subClassOf* cb:Model .\n",
    "      }}\n",
    "    }}\n",
    "'''\n",
    "            if ontology.query(query).askAnswer:\n",
    "                return i if return_index else io_shapes\n",
    "\n",
    "def identify_visual_io(ontology: Graph, ios: List[List[URIRef]], return_index: bool = False) -> Union[int, List[URIRef]]:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            query = f'''\n",
    "    PREFIX sh: <{SH}>\n",
    "    PREFIX rdfs: <{RDFS}>\n",
    "    PREFIX cb: <{cb}>\n",
    "\n",
    "    ASK {{\n",
    "      {{\n",
    "        {io_shape.n3()} sh:targetClass ?targetClass .\n",
    "        ?targetClass rdfs:subClassOf* cb:Visualization .\n",
    "      }}\n",
    "      UNION\n",
    "      {{\n",
    "        {io_shape.n3()} rdfs:subClassOf* cb:Visualization .\n",
    "      }}\n",
    "    }}\n",
    "'''\n",
    "            if ontology.query(query).askAnswer:\n",
    "                return i if return_index else io_shapes\n",
    "\n",
    "def satisfies_shape(data_graph: Graph, shacl_graph: Graph, shape: URIRef, focus: URIRef) -> bool:\n",
    "    conforms, g, report = validate(data_graph, shacl_graph=shacl_graph, validate_shapes=[shape], focus=focus)\n",
    "    return conforms\n",
    "\n",
    "def get_shape_target_class(ontology: Graph, shape: URIRef) -> URIRef:\n",
    "    return ontology.query(f\"\"\"\n",
    "        PREFIX sh: <{SH}>\n",
    "        SELECT ?targetClass\n",
    "        WHERE {{\n",
    "            <{shape}> sh:targetClass ?targetClass .\n",
    "        }}\n",
    "    \"\"\").bindings[0]['targetClass']\n",
    "\n",
    "\n",
    "def get_implementation_parameters(ontology: Graph, implementation: URIRef) -> Dict[\n",
    "    URIRef, Tuple[Literal, Literal, Literal]]:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?parameter ?value ?order ?condition\n",
    "        WHERE {{\n",
    "            <{implementation}> tb:hasParameter ?parameter .\n",
    "            ?parameter tb:has_defaultvalue ?value ;\n",
    "                       tb:has_condition ?condition ;\n",
    "                       tb:has_position ?order .\n",
    "        }}\n",
    "        ORDER BY ?order\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['value'], param['order'], param['condition']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_non_overriden_parameters(ontology: Graph, component: URIRef) -> Dict[\n",
    "    URIRef, Tuple[Literal, Literal, Literal]]:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?parameter ?parameterValue ?position ?condition\n",
    "        WHERE {{\n",
    "            {component.n3()} tb:hasImplementation ?implementation .\n",
    "            ?implementation tb:hasParameter ?parameter .\n",
    "            ?parameter tb:has_defaultvalue ?parameterValue ;\n",
    "                       tb:has_position ?position ;\n",
    "                       tb:has_condition ?condition .\n",
    "            FILTER NOT EXISTS {{\n",
    "                ?parameter tb:specifiedBy ?parameterSpecification .\n",
    "            }}\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['parameterValue'], param['position'], param['condition']) for param in results}\n",
    "\n",
    "\n",
    "\n",
    "def get_component_parameters(ontology: Graph, component: URIRef) -> Dict[URIRef, Tuple[Literal, Literal, Literal]]:\n",
    "    component_params = get_component_non_overriden_parameters(ontology, component)\n",
    "    return component_params\n",
    "\n",
    "\n",
    "\n",
    "def get_component_overridden_paramspecs(ontology: Graph, workflow_graph: Graph, component: URIRef) -> Dict[URIRef, Tuple[URIRef, Literal]]:\n",
    "    paramspecs_query = f\"\"\"\n",
    "\n",
    "        PREFIX tb:<{tb}>\n",
    "        SELECT ?parameterSpec ?parameter ?parameterValue ?position\n",
    "        WHERE{{\n",
    "            {component.n3()} tb:overridesParameter ?parameterSpec .\n",
    "            ?parameterSpec tb:hasValue ?parameterValue .\n",
    "            ?parameter tb:specifiedBy ?parameterSpec ;\n",
    "                       tb:has_position ?position .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(paramspecs_query).bindings\n",
    "\n",
    "    overridden_paramspecs = {paramspec['parameterSpec']: (paramspec['parameter'], paramspec['parameterValue'], paramspec['position']) for paramspec in results}\n",
    "\n",
    "    for paramspec, paramval_tup in overridden_paramspecs.items():\n",
    "        param, value, _ = paramval_tup\n",
    "        workflow_graph.add((paramspec, RDF.type, tb.ParameterSpecification))\n",
    "        workflow_graph.add((param, tb.specifiedBy, paramspec))\n",
    "        workflow_graph.add((paramspec, tb.hasValue, value))\n",
    "\n",
    "    return overridden_paramspecs\n",
    "\n",
    "\n",
    "\n",
    "def perform_param_substitution(graph: Graph, implementation: URIRef, parameters: Dict[URIRef, Tuple[Literal, Literal, Literal]],\n",
    "                               inputs: List[URIRef], intent_graph: Graph = None) -> Dict[URIRef, Tuple[Literal, Literal, Literal]]:\n",
    "    \n",
    "    parameter_keys = list(parameters.keys())\n",
    "    intent_parameters = get_intent_parameters(intent_graph) if intent_graph is not None else {}\n",
    "    intent_parameter_keys = list(intent_parameters.keys())\n",
    "\n",
    "    updated_parameters = parameters.copy()\n",
    "\n",
    "    for parameter, (default_value, position, condition) in parameters.items():\n",
    "        for parameter in intent_parameter_keys:\n",
    "            intent_value = intent_parameters[parameter]\n",
    "            updated_parameters[parameter] = (intent_value, position, condition)\n",
    "\n",
    "    parameters.update(updated_parameters)\n",
    "            \n",
    "\n",
    "    for param in parameter_keys:\n",
    "        value, order, condition = parameters[param]\n",
    "        if condition.value is not None and condition.value != '':\n",
    "            feature_types = get_inputs_feature_types(graph, inputs)\n",
    "            if condition.value == '$$INTEGER_COLUMN$$' and int not in feature_types:\n",
    "                parameters.pop(param)\n",
    "                continue\n",
    "            if condition.value == '$$STRING_COLUMN$$' and str not in feature_types:\n",
    "                parameters.pop(param)\n",
    "                continue\n",
    "            if condition.value == '$$FLOAT_COLUMN$$' and float not in feature_types:\n",
    "                parameters.pop(param)\n",
    "                continue\n",
    "        # if isinstance(value.value, str) and '$$LABEL$$' in value.value:\n",
    "        #     new_value = value.replace('$$LABEL$$', f'{get_inputs_label_name(graph, inputs)}')\n",
    "        #     parameters[param] = (Literal(new_value), order, condition)\n",
    "        if isinstance(value.value, str) and '$$NUMERIC_COLUMNS$$' in value.value:\n",
    "            new_value = value.replace('$$NUMERIC_COLUMNS$$', f'{get_inputs_numeric_columns(graph, inputs)}')\n",
    "            parameters[param] = (Literal(new_value), order, condition)\n",
    "        if isinstance(value.value, str) and '$$CSV_PATH$$' in value.value:\n",
    "            new_value = value.replace('$$CSV_PATH$$', f'{get_csv_path(graph, inputs)}')\n",
    "            parameters[param] = (Literal(new_value), order, condition)\n",
    "        if isinstance(value.value, str) and '&amp;' in value.value:\n",
    "            new_value = value.replace('&amp;', '&')\n",
    "            parameters[param] = (Literal(new_value), order, condition)\n",
    "        if isinstance(value.value, str) and value.value != '':\n",
    "            if condition.value == '$$BAR_EXCLUDED$$':\n",
    "                possible_cols = get_inputs_numeric_columns(graph, inputs)\n",
    "                included_cols = [ast.literal_eval(str(parameters[param][0])) for param in intent_parameters.keys() if 'included' in str(param)][0]\n",
    "                excluded_cols = list(set(possible_cols) - set(included_cols))\n",
    "                parameters[param] = (Literal(excluded_cols), order, condition)\n",
    "            elif condition.value == '$$HISTOGRAM_EXCLUDED$$':\n",
    "                possible_cols = get_inputs_numeric_columns(graph, inputs)\n",
    "                cat_col = [str(parameters[param][0]) for param in intent_parameters.keys() if 'histogram_category' in str(param)][0]\n",
    "                included_cols = [ast.literal_eval(str(parameters[param][0])) for param in intent_parameters.keys() if 'included' in str(param)][0]\n",
    "                excluded_cols = list(set(possible_cols) - set(cat_col) - set(included_cols))\n",
    "                parameters[param] = (Literal(excluded_cols), order, condition)\n",
    "            elif condition.value == '$$HEATMAP_EXCLUDED$$':\n",
    "                possible_cols = get_inputs_numeric_columns(graph, inputs)\n",
    "                included_cols = [ast.literal_eval(str(parameters[param][0])) for param in intent_parameters.keys() if 'included' in str(param)][0]\n",
    "                excluded_cols = list(set(possible_cols) - set(included_cols))\n",
    "                parameters[param] = (Literal(excluded_cols), order, condition)\n",
    "            elif condition.value == '$$LINEPLOT_EXCLUDED$$':\n",
    "                possible_cols = get_inputs_all_columns(graph, inputs) + ['<RowID>']\n",
    "                com_col = [str(parameters[param][0]) for param in intent_parameters.keys() if 'lineplot_x' in str(param)]\n",
    "                # print(f'POSSIBLE: {possible_cols}')\n",
    "                included_cols = [ast.literal_eval(str(parameters[param][0])) for param in intent_parameters.keys() if 'included' in str(param)][0]\n",
    "                # print(f'INCLUDED: {included_cols}')\n",
    "                excluded_cols = list(set(possible_cols) - set(com_col) - set(included_cols))\n",
    "                # print(f'EXCLUDED: {excluded_cols}')\n",
    "                parameters[param] = (Literal(excluded_cols), order, condition)\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def assign_to_parameter_specs(graph: Graph,\n",
    "                              parameters: Dict[URIRef, Tuple[Literal, Literal, Literal]])-> Dict[URIRef, Tuple[URIRef, Literal]]:\n",
    "    \n",
    "    keys = list(parameters.keys())\n",
    "    param_specs = {}\n",
    "    \n",
    "    for param in keys:\n",
    "\n",
    "        value, order, _ = parameters[param]\n",
    "        uri = param.split('#')[-1] if '#' in param else param.split('/')[-1]\n",
    "        sanitized_value = quote(value, safe=\"-_\")\n",
    "        sanitized_uri = URIRef(f'{uri}_{sanitized_value}_specification'.replace(' ','_').lower())\n",
    "        param_spec = ab.term(sanitized_uri)\n",
    "        # print(param_spec)\n",
    "        graph.add((param_spec, RDF.type, tb.ParameterSpecification))\n",
    "        graph.add((param, tb.specifiedBy, param_spec))\n",
    "        graph.add((param_spec, tb.hasValue, value))\n",
    "\n",
    "        param_specs[param_spec] = (param, value, order)\n",
    "    \n",
    "    return param_specs\n",
    "\n",
    "\n",
    "def add_step(graph: Graph, pipeline: URIRef, task_name: str, component: URIRef,\n",
    "             parameter_specs: Dict[URIRef, Tuple[URIRef, Literal]],\n",
    "             order: int, previous_task: Union[None, list, URIRef] = None, inputs: Optional[List[URIRef]] = None,\n",
    "             outputs: Optional[List[URIRef]] = None) -> URIRef:\n",
    "    if outputs is None:\n",
    "        outputs = []\n",
    "    if inputs is None:\n",
    "        inputs = []\n",
    "    step = ab.term(task_name)\n",
    "    graph.add((pipeline, tb.hasStep, step))\n",
    "    graph.add((step, RDF.type, tb.Step))\n",
    "    graph.add((step, tb.runs, component))\n",
    "    graph.add((step, tb.has_position, Literal(order)))\n",
    "    for i, input in enumerate(inputs):\n",
    "        in_node = BNode()\n",
    "        graph.add((in_node, RDF.type, tb.Data))\n",
    "        graph.add((in_node, tb.has_data, input))\n",
    "        graph.add((in_node, tb.has_position, Literal(i)))\n",
    "        graph.add((step, tb.hasInput, in_node))\n",
    "    for o, output in enumerate(outputs):\n",
    "        out_node = BNode()\n",
    "        graph.add((out_node, RDF.type, tb.Data))\n",
    "        graph.add((out_node, tb.has_data, output))\n",
    "        graph.add((out_node, tb.has_position, Literal(o)))\n",
    "        graph.add((step, tb.hasOutput, out_node))\n",
    "    for param, *_ in parameter_specs.values():\n",
    "        graph.add((step, tb.usesParameter, param))\n",
    "    if previous_task:\n",
    "        if isinstance(previous_task, list):\n",
    "            for previous in previous_task:\n",
    "                graph.add((previous, tb.followedBy, step))\n",
    "        else:\n",
    "            graph.add((previous_task, tb.followedBy, step))\n",
    "    return step\n",
    "\n",
    "\n",
    "def get_component_transformations(ontology: Graph, component: URIRef) -> List[URIRef]:\n",
    "    transformation_query = f'''\n",
    "        PREFIX tb: <{tb}>\n",
    "        SELECT ?transformation\n",
    "        WHERE {{\n",
    "            <{component}> tb:hasTransformation ?transformation_list .\n",
    "            ?transformation_list rdf:rest*/rdf:first ?transformation .\n",
    "        }}\n",
    "    '''\n",
    "    transformations = ontology.query(transformation_query).bindings\n",
    "    return [x['transformation'] for x in transformations]\n",
    "\n",
    "\n",
    "def get_inputs_all_columns(graph: Graph, inputs: List[URIRef]) -> List:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    columns_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isFeature true ;\n",
    "                    dmop:hasDataPrimitiveTypeColumn ?type ;\n",
    "                    dmop:hasColumnName ?label .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    columns = graph.query(columns_query).bindings\n",
    "    return [x['label'].value for x in columns]\n",
    "\n",
    "\n",
    "def get_inputs_label_name(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    \n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "\n",
    "    label_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isLabel true ;\n",
    "                    dmop:hasColumnName ?label .\n",
    "\n",
    "        }}\n",
    "    \"\"\"\n",
    "    \n",
    "    results = graph.query(label_query).bindings\n",
    "    \n",
    "    if results is not None and len(results) > 0:\n",
    "        return results[0]['label']\n",
    "    \n",
    "\n",
    "def get_exact_column(graph: Graph, inputs: List[URIRef], column_name: str) -> str:\n",
    "    \n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "\n",
    "    column_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:hasColumnName ?label .\n",
    "            FILTER(?label = \"{column_name}\")\n",
    "        }}\n",
    "    \"\"\"\n",
    "    \n",
    "    results = graph.query(column_query).bindings\n",
    "    \n",
    "    if results is not None and len(results) > 0:\n",
    "        return results[0]['label']\n",
    "\n",
    "\n",
    "def get_inputs_numeric_columns(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    columns_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isFeature true ;\n",
    "                    dmop:hasDataPrimitiveTypeColumn ?type ;\n",
    "                    dmop:hasColumnName ?label .\n",
    "            FILTER(?type IN (dmop:Float, dmop:Int, dmop:Number, dmop:Double, dmop:Long, dmop:Short, dmop:Integer))\n",
    "        }}\n",
    "    \"\"\"\n",
    "    columns = graph.query(columns_query).bindings\n",
    " \n",
    "    return [x['label'].value for x in columns]\n",
    "\n",
    "\n",
    "def get_inputs_categorical_columns(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "\n",
    "    categ_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isCategorical true ;\n",
    "                    dmop:hasDataPrimitiveTypeColumn ?type ;\n",
    "                    dmop:hasColumnName ?label .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    columns = graph.query(categ_query).bindings\n",
    "\n",
    "    return [x['label'].value for x in columns]\n",
    "\n",
    "\n",
    "def get_csv_path(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    path = next(graph.objects(data_input, dmop.path), True)\n",
    "    return path.value\n",
    "\n",
    "\n",
    "def get_inputs_feature_types(graph: Graph, inputs: List[URIRef]) -> Set[Type]:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    columns_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?type\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isFeature true ;\n",
    "                    dmop:hasDataPrimitiveTypeColumn ?type .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    columns = graph.query(columns_query).bindings\n",
    "    mapping = {\n",
    "        dmop.Float: float,\n",
    "        dmop.Int: int,\n",
    "        dmop.Integer: int,\n",
    "        dmop.Number: float,\n",
    "        dmop.Double: float,\n",
    "        dmop.String: str,\n",
    "        dmop.Boolean: bool,\n",
    "    }\n",
    "    return set([mapping[x['type']] for x in columns])\n",
    "\n",
    "\n",
    "def copy_subgraph(source_graph: Graph, source_node: URIRef, destination_graph: Graph, destination_node: URIRef,\n",
    "                  replace_nodes: bool = True) -> None:\n",
    "    visited_nodes = set()\n",
    "    nodes_to_visit = [source_node]\n",
    "    mappings = {source_node: destination_node}\n",
    "\n",
    "    while nodes_to_visit:\n",
    "        current_node = nodes_to_visit.pop()\n",
    "        visited_nodes.add(current_node)\n",
    "        for predicate, object in source_graph.predicate_objects(current_node):\n",
    "            if predicate == OWL.sameAs:\n",
    "                continue\n",
    "            if replace_nodes and isinstance(object, IdentifiedNode):\n",
    "                if predicate == RDF.type or object in dmop:\n",
    "                    mappings[object] = object\n",
    "                else:\n",
    "                    if object not in visited_nodes:\n",
    "                        nodes_to_visit.append(object)\n",
    "                    if object not in mappings:\n",
    "                        mappings[object] = BNode()\n",
    "                destination_graph.add((mappings[current_node], predicate, mappings[object]))\n",
    "            else:\n",
    "                destination_graph.add((mappings[current_node], predicate, object))\n",
    "\n",
    "\n",
    "def annotate_io_with_spec(ontology: Graph, workflow_graph: Graph, io: URIRef, io_spec: List[URIRef]) -> None:\n",
    "    \n",
    "    for spec in io_spec:\n",
    "        io_spec_class = next(ontology.objects(spec, SH.targetClass, True), None)\n",
    "        if io_spec_class is None or (io, RDF.type, io_spec_class) in workflow_graph:\n",
    "            continue\n",
    "        workflow_graph.add((io, RDF.type, io_spec_class))\n",
    "\n",
    "\n",
    "def annotate_ios_with_specs(ontology: Graph, workflow_graph: Graph, data_io: List[URIRef],\n",
    "                            specs: List[List[URIRef]]) -> None:\n",
    "    assert len(data_io) == len(specs), 'Number of IOs and specs must be the same'\n",
    "    for io, spec in zip(data_io, specs):\n",
    "        annotate_io_with_spec(ontology, workflow_graph, io, spec)\n",
    "\n",
    "\n",
    "def run_copy_transformation(ontology: Graph, workflow_graph: Graph, transformation: URIRef, inputs: List[URIRef],\n",
    "                            outputs: List[URIRef]):\n",
    "    input_index = next(ontology.objects(transformation, tb.copy_input, True)).value\n",
    "    output_index = next(ontology.objects(transformation, tb.copy_output, True)).value\n",
    "    input = inputs[input_index - 1]\n",
    "    output = outputs[output_index - 1]\n",
    "\n",
    "    copy_subgraph(workflow_graph, input, workflow_graph, output)\n",
    "\n",
    "\n",
    "def run_component_transformation(ontology: Graph, workflow_graph: Graph, component: URIRef, inputs: List[URIRef],\n",
    "                                 outputs: List[URIRef],\n",
    "                                 parameters_specs: Dict[URIRef, Tuple[URIRef, Literal, Literal]]) -> None:\n",
    "    transformations = get_component_transformations(ontology, component)\n",
    "    for transformation in transformations:\n",
    "        if (transformation, RDF.type, tb.CopyTransformation) in ontology:\n",
    "            run_copy_transformation(ontology, workflow_graph, transformation, inputs, outputs)\n",
    "        elif (transformation, RDF.type, tb.LoaderTransformation) in ontology:\n",
    "            continue\n",
    "        else:\n",
    "            prefixes = f'''\n",
    "PREFIX tb: <{tb}>\n",
    "PREFIX ab: <{ab}>\n",
    "PREFIX rdf: <{RDF}>\n",
    "PREFIX rdfs: <{RDFS}>\n",
    "PREFIX owl: <{OWL}>\n",
    "PREFIX xsd: <{XSD}>\n",
    "PREFIX dmop: <{dmop}>\n",
    "'''\n",
    "            query = next(ontology.objects(transformation, tb.transformation_query, True)).value\n",
    "            query = prefixes + query\n",
    "            for i in range(len(inputs)):\n",
    "                query = query.replace(f'$input{i + 1}', f'{inputs[i].n3()}')\n",
    "            for i in range(len(outputs)):\n",
    "                query = query.replace(f'$output{i + 1}', f'{outputs[i].n3()}')\n",
    "            for param_spec, (param, value, order) in parameters_specs.items():\n",
    "                query = query.replace(f'$param{order + 1}', f'{value.n3()}')\n",
    "                query = query.replace(f'$parameter{order + 1}', f'{value.n3()}')\n",
    "            workflow_graph.update(query)\n",
    "\n",
    "\n",
    "def retreive_component_rules(graph: Graph, task:URIRef, component: URIRef):\n",
    "    preference_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "\n",
    "        SELECT ?datatag ?weight ?component_rank\n",
    "        WHERE {{\n",
    "            {component.n3()} tb:hasRule ?rule .\n",
    "            ?rule tb:relatedtoDatatag ?datatag ;\n",
    "                  tb:relatedtoTask {task.n3()} ;\n",
    "                  tb:has_rank ?component_rank .\n",
    "            ?datatag tb:has_weight ?weight .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = graph.query(preference_query).bindings\n",
    "\n",
    "    return {result['datatag']: (float(result['weight']), int(result['component_rank'])) for result in results}\n",
    "\n",
    "\n",
    "def get_best_components(graph: Graph, task: URIRef, components: List[URIRef], dataset: URIRef, percentage: float = None):\n",
    "\n",
    "    preferred_components = {}\n",
    "    sorted_components = {}\n",
    "    for component in components:\n",
    "        \n",
    "        component_rules = retreive_component_rules(graph, task, component)\n",
    "        score = 0\n",
    "\n",
    "        preferred_components[component] = score\n",
    "\n",
    "        for datatag, weight_rank in component_rules.items():\n",
    "            rule_weight = weight_rank[0]\n",
    "            component_rank = weight_rank[1]\n",
    "            if satisfies_shape(graph, graph, datatag, dataset):\n",
    "                score+=rule_weight\n",
    "            else:\n",
    "                score-=rule_weight\n",
    "                \n",
    "            preferred_components[component] = (score, component_rank)\n",
    "        \n",
    "    sorted_preferred = sorted(preferred_components.items(), key=lambda x: x[1][0], reverse=True)\n",
    "\n",
    "    if len(sorted_preferred) > 0: ### there are multiple components to choose from\n",
    "        best_scores = set([comp[1] for comp in sorted_preferred])\n",
    "        print(f'Task: {task}')\n",
    "        print(f'SP: {sorted_preferred}')\n",
    "        print(f'SCORES: {best_scores}')\n",
    "        if len(best_scores) == 1:\n",
    "            sorted_preferred = random.sample(sorted_preferred, int(math.ceil(len(sorted_preferred)*percentage))) if percentage else sorted_preferred\n",
    "        elif len(best_scores) > 1: ### checking if there is at least one superior component\n",
    "            sorted_preferred = [x for x in sorted_preferred if x[1] >= sorted_preferred[0][1]]\n",
    "\n",
    "\n",
    "    for comp, rules_nbr in sorted_preferred:\n",
    "        sorted_components[comp] = rules_nbr \n",
    "\n",
    "    return sorted_components\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def get_step_name(workflow_name: str, task_order: int, implementation: URIRef) -> str:\n",
    "    return f'{workflow_name}-step_{task_order}_{implementation.fragment.replace(\"-\", \"_\")}'\n",
    "\n",
    "\n",
    "def add_loader_step(ontology: Graph, workflow_graph: Graph, workflow: URIRef, dataset_node: URIRef) -> URIRef:\n",
    "    loader_component = cb.term('component-csv_local_reader')\n",
    "    loader_step_name = get_step_name(workflow.fragment, 0, loader_component)\n",
    "    loader_parameters = get_component_parameters(ontology, loader_component)\n",
    "    loader_overridden_paramspecs = get_component_overridden_paramspecs(ontology, workflow_graph, loader_component)\n",
    "    loader_parameters = perform_param_substitution(workflow_graph, None, loader_parameters, [dataset_node])\n",
    "    loader_param_specs = assign_to_parameter_specs(workflow_graph, loader_parameters)\n",
    "    loader_param_specs.update(loader_overridden_paramspecs)\n",
    "    return add_step(workflow_graph, workflow, loader_step_name, loader_component, loader_param_specs, 0, None, None,\n",
    "                    [dataset_node])\n",
    "\n",
    "\n",
    "def build_general_workflow(workflow_name: str, ontology: Graph, dataset: URIRef, main_component: URIRef,\n",
    "                           transformations: List[URIRef], intent_graph:Graph) -> Tuple[Graph, URIRef]:\n",
    "    workflow_graph = get_graph_xp()\n",
    "    workflow = ab.term(workflow_name)\n",
    "    workflow_graph.add((workflow, RDF.type, tb.Workflow))\n",
    "    task_order = 0\n",
    "\n",
    "    dataset_node = ab.term(f'{workflow_name}-original_dataset')\n",
    "\n",
    "    copy_subgraph(ontology, dataset, workflow_graph, dataset_node)\n",
    "\n",
    "    loader_step = add_loader_step(ontology, workflow_graph, workflow, dataset_node)\n",
    "    task_order += 1\n",
    "\n",
    "    previous_step = loader_step\n",
    "    previous_train_step = loader_step\n",
    "    previous_test_step = None\n",
    "\n",
    "    previous_node = dataset_node\n",
    "    train_dataset_node = dataset_node\n",
    "    test_dataset_node = None\n",
    "\n",
    "\n",
    "    for train_component in [*transformations, main_component]:\n",
    "    \n",
    "        test_component = next(ontology.objects(train_component, tb.hasApplier, True), None)\n",
    "        same = train_component == test_component\n",
    "        train_component_implementation = get_component_implementation(ontology, train_component)\n",
    "\n",
    "\n",
    "        if not test_component:\n",
    "            \n",
    "            singular_component = train_component\n",
    "            singular_step_name = get_step_name(workflow_name, task_order, singular_component)\n",
    "            singular_component_implementation = get_component_implementation(ontology, singular_component)\n",
    "            singular_input_specs = get_implementation_input_specs(ontology, singular_component_implementation)\n",
    "            singular_input_data_index = identify_data_io(ontology, singular_input_specs, return_index=True)\n",
    "            singular_transformation_inputs = None\n",
    "            if singular_input_data_index is not None:\n",
    "                singular_transformation_inputs = [ab[f'{singular_step_name}-input_{i}'] for i in range(len(singular_input_specs))]\n",
    "                singular_transformation_inputs[singular_input_data_index] = previous_node\n",
    "                annotate_ios_with_specs(ontology, workflow_graph, singular_transformation_inputs,\n",
    "                                        singular_input_specs)\n",
    "            singular_output_specs = get_implementation_output_specs(ontology, singular_component_implementation)\n",
    "            singular_transformation_outputs = [ab[f'{singular_step_name}-output_{i}'] for i in range(len(singular_output_specs))]\n",
    "            annotate_ios_with_specs(ontology, workflow_graph, singular_transformation_outputs,\n",
    "                                singular_output_specs)\n",
    "            \n",
    "            singular_parameters = get_component_parameters(ontology, singular_component)\n",
    "            singular_overridden_parameters = get_component_overridden_paramspecs(ontology, workflow_graph, singular_component)\n",
    "            singular_parameters = perform_param_substitution(graph=workflow_graph, parameters=singular_parameters,\n",
    "                                                                implementation=singular_component_implementation,\n",
    "                                                                inputs=singular_transformation_inputs,\n",
    "                                                                intent_graph=intent_graph)\n",
    "            singular_param_specs = assign_to_parameter_specs(workflow_graph, singular_parameters)\n",
    "            singular_param_specs.update(singular_overridden_parameters)\n",
    "            singular_step = add_step(workflow_graph, workflow,\n",
    "                                singular_step_name,\n",
    "                                singular_component,\n",
    "                                singular_param_specs,\n",
    "                                task_order, previous_step,\n",
    "                                [previous_node],\n",
    "                                singular_transformation_outputs)\n",
    "            run_component_transformation(ontology, workflow_graph, singular_component,\n",
    "                                            [previous_node], singular_transformation_outputs, singular_param_specs)\n",
    "\n",
    "\n",
    "            train_dataset_index = identify_data_io(ontology, singular_output_specs, train=True, return_index=True)\n",
    "            \n",
    "            test_dataset_index = identify_data_io(ontology, singular_output_specs, test=True, return_index=True)\n",
    "\n",
    "            if train_dataset_index is not None:\n",
    "                train_dataset_node =  singular_transformation_outputs[train_dataset_index]\n",
    "            if test_dataset_index is not None:\n",
    "                test_dataset_node = singular_transformation_outputs[test_dataset_index]\n",
    "                \n",
    "            previous_step = singular_step\n",
    "            previous_train_step = singular_step\n",
    "            previous_test_step = singular_step\n",
    "            \n",
    "            task_order += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            train_step_name = get_step_name(workflow_name, task_order, train_component)\n",
    "\n",
    "            train_component_implementation = get_component_implementation(ontology, train_component)\n",
    "\n",
    "            train_input_specs = get_implementation_input_specs(ontology, train_component_implementation)\n",
    "            train_input_data_index = identify_data_io(ontology, train_input_specs, return_index=True)\n",
    "            train_transformation_inputs = [ab[f'{train_step_name}-input_{i}'] for i in range(len(train_input_specs))]\n",
    "            train_transformation_inputs[train_input_data_index] = train_dataset_node \n",
    "            annotate_ios_with_specs(ontology, workflow_graph, train_transformation_inputs,\n",
    "                                    train_input_specs)\n",
    "\n",
    "            train_output_specs = get_implementation_output_specs(ontology, train_component_implementation)\n",
    "            train_output_model_index = identify_model_io(ontology, train_output_specs, return_index=True)\n",
    "            train_output_data_index = identify_data_io(ontology, train_output_specs, return_index=True)\n",
    "            train_transformation_outputs = [ab[f'{train_step_name}-output_{i}'] for i in range(len(train_output_specs))]\n",
    "            annotate_ios_with_specs(ontology, workflow_graph, train_transformation_outputs,\n",
    "                                    train_output_specs)\n",
    "\n",
    "            train_parameters = get_component_parameters(ontology, train_component)\n",
    "            train_parameters = perform_param_substitution(graph=workflow_graph, implementation=train_component_implementation,\n",
    "                                                            parameters=train_parameters,\n",
    "                                                            intent_graph=intent_graph,\n",
    "                                                            inputs=train_transformation_inputs)\n",
    "            train_overridden_paramspecs = get_component_overridden_paramspecs(ontology, workflow_graph, train_component)\n",
    "            train_param_specs = assign_to_parameter_specs(workflow_graph, train_parameters)\n",
    "            train_param_specs.update(train_overridden_paramspecs)\n",
    "            train_step = add_step(workflow_graph, workflow,\n",
    "                                train_step_name,\n",
    "                                train_component,\n",
    "                                train_param_specs,\n",
    "                                task_order, previous_train_step,\n",
    "                                train_transformation_inputs,\n",
    "                                train_transformation_outputs)\n",
    "\n",
    "            previous_train_step = train_step \n",
    "\n",
    "            run_component_transformation(ontology, workflow_graph, train_component, train_transformation_inputs,\n",
    "                                        train_transformation_outputs, train_param_specs)\n",
    "\n",
    "            if train_output_data_index is not None:\n",
    "                train_dataset_node = train_transformation_outputs[train_output_data_index]\n",
    "\n",
    "            previous_step = train_step\n",
    "            previous_node = train_dataset_node\n",
    "\n",
    "            task_order += 1\n",
    "\n",
    "            if test_dataset_node is not None:\n",
    "\n",
    "                test_step_name = get_step_name(workflow_name, task_order, test_component)\n",
    "\n",
    "                test_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                                get_component_implementation(ontology, test_component))\n",
    "                test_input_data_index = identify_data_io(ontology, test_input_specs, test=True, return_index=True)\n",
    "                test_input_model_index = identify_model_io(ontology, test_input_specs, return_index=True)\n",
    "                test_transformation_inputs = [ab[f'{test_step_name}-input_{i}'] for i in range(len(test_input_specs))]\n",
    "                test_transformation_inputs[test_input_data_index] = test_dataset_node\n",
    "                if train_output_model_index is not None:\n",
    "                    test_transformation_inputs[test_input_model_index] = train_transformation_outputs[train_output_model_index]\n",
    "                annotate_ios_with_specs(ontology, workflow_graph, test_transformation_inputs,\n",
    "                                        test_input_specs)\n",
    "                \n",
    "                test_implementation = get_component_implementation(ontology, test_component)\n",
    "                test_output_specs = get_implementation_output_specs(ontology, test_implementation)\n",
    "                test_output_data_index = identify_data_io(ontology, test_output_specs, return_index=True)\n",
    "                test_transformation_outputs = [ab[f'{test_step_name}-output_{i}'] for i in range(len(test_output_specs))]\n",
    "                annotate_ios_with_specs(ontology, workflow_graph, test_transformation_outputs,\n",
    "                                        test_output_specs)\n",
    "\n",
    "                previous_test_steps = [previous_test_step, train_step] if not same else [previous_test_step]\n",
    "                test_parameters = get_component_parameters(ontology, test_component)\n",
    "                test_component_implementation = get_component_implementation(ontology, test_component) if test_component else None\n",
    "                test_parameters = perform_param_substitution(workflow_graph,\n",
    "                                                             test_component_implementation,\n",
    "                                                             test_parameters,\n",
    "                                                             test_transformation_inputs,\n",
    "                                                             intent_graph=intent_graph)\n",
    "                test_overridden_paramspecs = get_component_overridden_paramspecs(ontology, workflow_graph, train_component)\n",
    "                test_param_specs = assign_to_parameter_specs(workflow_graph, test_parameters)\n",
    "                test_param_specs.update(test_overridden_paramspecs)\n",
    "                test_step = add_step(workflow_graph, workflow,\n",
    "                                    test_step_name,\n",
    "                                    test_component,\n",
    "                                    test_param_specs,\n",
    "                                    task_order, previous_test_steps,\n",
    "                                    test_transformation_inputs,\n",
    "                                    test_transformation_outputs)\n",
    "\n",
    "                run_component_transformation(ontology, workflow_graph, test_component, test_transformation_inputs,\n",
    "                                            test_transformation_outputs, test_param_specs)\n",
    "                \n",
    "                # print(f'TEST OUTPUT{test_output_data_index}: {test_transformation_outputs}')\n",
    "                test_dataset_node = test_transformation_outputs[test_output_data_index]\n",
    "                previous_test_step = test_step\n",
    "                task_order += 1\n",
    "            \n",
    "    \n",
    "    if test_dataset_node is not None:\n",
    "        saver_component = cb.term('component-csv_local_writer')\n",
    "        saver_step_name = get_step_name(workflow_name, task_order, saver_component)\n",
    "        saver_parameters = get_component_parameters(ontology, saver_component)\n",
    "        saver_implementation = get_component_implementation(ontology, saver_component)\n",
    "        saver_parameters = perform_param_substitution(workflow_graph, saver_implementation, saver_parameters, [test_dataset_node])\n",
    "        saver_overridden_paramspecs = get_component_overridden_paramspecs(ontology, workflow_graph, saver_component)\n",
    "        # print(f'SAVE: {saver_component}: {saver_parameters}')\n",
    "        saver_param_specs = assign_to_parameter_specs(workflow_graph, saver_parameters)\n",
    "        saver_param_specs.update(saver_overridden_paramspecs)\n",
    "        add_step(workflow_graph,\n",
    "                workflow,\n",
    "                saver_step_name,\n",
    "                saver_component,\n",
    "                saver_param_specs,\n",
    "                task_order,\n",
    "                previous_test_step, [test_dataset_node], [])\n",
    "\n",
    "    return workflow_graph, workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No folder introduced, using default (./workflows/2024-12-04 05-09-54/)\n",
      "Directory does not exist, creating it\n",
      "Intent: VisualizationIntent\n",
      "Dataset: titanic.csv\n",
      "Task: DataVisualization\n",
      "Algorithm: HeatMap\n",
      "Preprocessing Component Percentage Threshold: 50.0%\n",
      "-------------------------------------------------\n",
      "Component: component-heatmap_visualizer (implementation-heatmap_visualizer)\n",
      "\tInput: ['NonNullTabularDatasetShape', 'NormalizedTabularDatasetShape', 'TabularDataset']\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              \n",
      "\u001b[A                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component: component-heatmap_visualizer (implementation-heatmap_visualizer)\n",
      "\tData input: ['NonNullTabularDatasetShape', 'NormalizedTabularDatasetShape', 'TabularDataset']\n",
      "UNSATISFIED SHAPES: [rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#NonNullTabularDatasetShape'), rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#NormalizedTabularDatasetShape')]\n",
      "AVAILABLE TRANSFORMATIONS: {rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#NonNullTabularDatasetShape'): [rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-drop_rows_with_missing_values'), rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-mean_imputation')], rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#NormalizedTabularDatasetShape'): [rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-decimal_scaling'), rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-min_max_scaling'), rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-z_score_scaling')]}\n",
      "Task: https://extremexp.eu/ontology/cbox#DataVisualization\n",
      "SP: [(rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-drop_rows_with_missing_values'), (1.0, 1)), (rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-mean_imputation'), (1.0, 1))]\n",
      "SCORES: {(1.0, 1)}\n",
      "Task: https://extremexp.eu/ontology/cbox#DataVisualization\n",
      "SP: [(rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-decimal_scaling'), (1.0, 2)), (rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-min_max_scaling'), (1.0, 1)), (rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-z_score_scaling'), (1.0, 1))]\n",
      "SCORES: {(1.0, 1), (1.0, 2)}\n",
      "REFINED TRANSFORMATIONS: {rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#NonNullTabularDatasetShape'): [rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-drop_rows_with_missing_values')], rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#NormalizedTabularDatasetShape'): [rdflib.term.URIRef('https://extremexp.eu/ontology/cbox#component-decimal_scaling')]}\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values']\n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling']\n",
      "\tTotal combinations: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transformations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Transformations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tCombination 1 / 1: ['component-drop_rows_with_missing_values', 'component-decimal_scaling']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Components: 100%|| 1/1 [00:00<00:00,  2.64it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tWorkflow 0: workflow_0_VisualizationIntent_c5030f2c_e906_4439_afdc_bf029245b1cd\n",
      "Workflows built in 7.437650442123413 seconds\n",
      "Workflows saved in ./workflows/2024-12-04 05-09-54/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_workflows(ontology: Graph, intent_graph: Graph, destination_folder: str, log: bool = False) -> None:\n",
    "    \n",
    "    dataset, task, algorithm, intent_iri = get_intent_info(intent_graph)\n",
    "\n",
    "    component_threshold = float(next(intent_graph.objects(intent_iri, tb.has_component_threshold), None))\n",
    "\n",
    "    if log:\n",
    "        tqdm.write(f'Intent: {intent_iri.fragment}')\n",
    "        tqdm.write(f'Dataset: {dataset.fragment}')\n",
    "        tqdm.write(f'Task: {task.fragment}')\n",
    "        tqdm.write(f'Algorithm: {algorithm.fragment if algorithm is not None else [algo.fragment for algo in get_algorithms_from_task(ontology, task)]}')\n",
    "        tqdm.write(f'Preprocessing Component Percentage Threshold: {component_threshold*100}%')\n",
    "        tqdm.write('-------------------------------------------------')\n",
    "\n",
    "    all_cols = get_inputs_all_columns(ontology, [dataset])\n",
    "    cat_cols = get_inputs_categorical_columns(ontology, [dataset])\n",
    "    num_cols = get_inputs_numeric_columns(ontology, [dataset])\n",
    "    \n",
    "    exp_params = get_exposed_parameters(ontology, task, algorithm)\n",
    "\n",
    "    for exp_param in exp_params:\n",
    "        option_columns = []\n",
    "        if 'CATEGORICAL' in exp_param['value']:\n",
    "            option_columns = cat_cols\n",
    "        elif 'NUMERICAL' in exp_param['value']:\n",
    "            option_columns = num_cols\n",
    "        else:\n",
    "            option_columns = all_cols\n",
    "\n",
    "        if 'COMPLETE' in exp_param['value']:\n",
    "            option_columns.append('<RowID>')\n",
    "\n",
    "        if 'INCLUDED' in exp_param['condition']:\n",
    "            param_val = []\n",
    "            col_num = int(input(f\"How many columns do you want to enter for {exp_param['label']} parameter?\"))\n",
    "            for i in range(col_num):\n",
    "                param_val.append(input(f\"Enter a value for {exp_param['label']} from the following: {option_columns}\"))\n",
    "        else:\n",
    "            param_val = input(f\"Enter a value for {exp_param['label']} from the following: {option_columns}\")\n",
    "\n",
    "        intent_graph.add((intent_iri, tb.specifiesValue, Literal(param_val)))\n",
    "        intent_graph.add((Literal(param_val), tb.forParameter, exp_param['exp_param']))\n",
    "\n",
    "\n",
    "    impls = get_potential_implementations(ontology, task, algorithm)\n",
    "    components = [\n",
    "        (c, impl, inputs)\n",
    "        for impl, inputs in impls\n",
    "        for c in get_implementation_components(ontology, impl)\n",
    "    ]\n",
    "\n",
    "\n",
    "    if log:\n",
    "        for component, implementation, inputs in components:\n",
    "            tqdm.write(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "            for im_input in inputs:\n",
    "                tqdm.write(f'\\tInput: {[x.fragment for x in im_input]}')\n",
    "        tqdm.write('-------------------------------------------------')\n",
    "\n",
    "    workflow_order = 0\n",
    "\n",
    "    for component, implementation, inputs in tqdm(components, desc='Components', position=1):\n",
    "        if log:\n",
    "            tqdm.write(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "        shapes_to_satisfy = identify_data_io(ontology, inputs)\n",
    "        assert shapes_to_satisfy is not None and len(shapes_to_satisfy) > 0\n",
    "        if log:\n",
    "            tqdm.write(f'\\tData input: {[x.fragment for x in shapes_to_satisfy]}')\n",
    "\n",
    "        unsatisfied_shapes = [shape for shape in shapes_to_satisfy if\n",
    "                              not satisfies_shape(ontology, ontology, shape, dataset)]\n",
    "        print(f'UNSATISFIED SHAPES: {unsatisfied_shapes}')\n",
    "        available_transformations = {\n",
    "            shape: find_components_to_satisfy_shape(ontology, shape, exclude_appliers=True)\n",
    "            for shape in unsatisfied_shapes\n",
    "        }\n",
    "        print(f'AVAILABLE TRANSFORMATIONS: {available_transformations}')\n",
    "\n",
    "        for tr, methods in available_transformations.items():\n",
    "\n",
    "            best_components = get_best_components(ontology, task, methods, dataset, component_threshold)\n",
    "\n",
    "            available_transformations[tr] = list(best_components.keys())\n",
    "\n",
    "        print(f'REFINED TRANSFORMATIONS: {available_transformations}')\n",
    "                    \n",
    "\n",
    "\n",
    "        if log:\n",
    "            tqdm.write(f'\\tUnsatisfied shapes: ')\n",
    "            for shape, transformations in available_transformations.items():\n",
    "                tqdm.write(f'\\t\\t{shape.fragment}: {[x.fragment for x in transformations]}')\n",
    "\n",
    "        transformation_combinations = list(\n",
    "            enumerate(itertools.product(*available_transformations.values())))\n",
    "            \n",
    "        # TODO - check if the combination is valid and whether further transformations are needed\n",
    "\n",
    "        if log:\n",
    "            tqdm.write(f'\\tTotal combinations: {len(transformation_combinations)}')\n",
    "\n",
    "        for i, transformation_combination in tqdm(transformation_combinations, desc='Transformations', position=0,\n",
    "                                                  leave=False):\n",
    "            if log:\n",
    "                tqdm.write(\n",
    "                    f'\\t\\tCombination {i + 1} / {len(transformation_combinations)}: {[x.fragment for x in transformation_combination]}')\n",
    "\n",
    "            workflow_name = f'workflow_{workflow_order}_{intent_iri.fragment}_{uuid.uuid4()}'.replace('-', '_')\n",
    "\n",
    "            wg, w = build_general_workflow(workflow_name, ontology, dataset, component,\n",
    "                                           transformation_combination, intent_graph = intent_graph)\n",
    "\n",
    "            wg.add((w, tb.generatedFor, intent_iri))\n",
    "            wg.add((intent_iri, RDF.type, tb.Intent))\n",
    "\n",
    "            if log:\n",
    "                tqdm.write(f'\\t\\tWorkflow {workflow_order}: {w.fragment}')\n",
    "            wg.serialize(os.path.join(destination_folder, f'{workflow_name}.ttl'), format='turtle')\n",
    "            workflow_order += 1\n",
    "\n",
    "def interactive():\n",
    "    intent_graph = get_graph_xp()\n",
    "    intent = input('Introduce the intent name [ClassificationIntent]: ') or 'VisualizationIntent' #or 'ClassificationIntent'\n",
    "    data = input('Introduce the dataset name [titanic.csv]: ') or 'titanic.csv'\n",
    "    task = input('Introduce the task name [Classification]: ') or  'DataVisualization' #or 'Classification'\n",
    "\n",
    "\n",
    "    intent_graph.add((ab.term(intent), RDF.type, tb.Intent))\n",
    "    intent_graph.add((ab.term(intent), tb.overData, ab.term(data)))\n",
    "    intent_graph.add((cb.term(task), tb.tackles, ab.term(intent)))\n",
    "\n",
    "\n",
    "    ontology = get_ontology_graph()\n",
    "\n",
    "    if task == 'DataVisualization':\n",
    "        algos = [alg.fragment for alg in get_algorithms_from_task(ontology, cb.term(task))]\n",
    "        vis_algorithm = str(input(f'Choose a visualization algorithm from the following (case-sensitive):{algos}'))\n",
    "        if vis_algorithm is not None:\n",
    "            intent_graph.add((ab.term(intent), tb.specifies, cb.term(vis_algorithm)))\n",
    "\n",
    "    component_percentage = float(input('Choose a threshold component percentage (for the preprocessing components) (%): [100, 75, 50, 25]') or 100)/100.0\n",
    "\n",
    "    intent_graph.add((ab.term(intent), tb.has_component_threshold, Literal(component_percentage)))\n",
    "\n",
    "    folder = input('Introduce the folder to save the workflows: ')\n",
    "    if folder == '':\n",
    "        folder = f'./workflows/{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}/'\n",
    "        tqdm.write(f'No folder introduced, using default ({folder})')\n",
    "    if not os.path.exists(folder):\n",
    "        tqdm.write('Directory does not exist, creating it')\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    t = time.time()\n",
    "    build_workflows(ontology, intent_graph, folder, log=True)\n",
    "    t = time.time() - t\n",
    "\n",
    "    print(f'Workflows built in {t} seconds')\n",
    "    print(f'Workflows saved in {folder}')\n",
    "\n",
    "\n",
    "interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
